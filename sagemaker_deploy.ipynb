{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d77c3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "# from sagemaker.image_uris import retrieve\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ed98a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'seteamperformance'\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7130fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error: An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if my_region == 'us-east-2':\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': my_region})\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35d43ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://seteamperformance/xgboost-as-a-built-in-algo/output\n"
     ]
    }
   ],
   "source": [
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "output_path = 's3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d75f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    averageResponsesByStudent  SE Process grade\n",
      "0                     10.1429                 1\n",
      "1                      9.5714                 0\n",
      "2                     10.5000                 0\n",
      "3                     11.1667                 0\n",
      "4                      9.1667                 1\n",
      "..                        ...               ...\n",
      "69                    11.1667                 0\n",
      "70                    12.0000                 1\n",
      "71                    11.3333                 0\n",
      "72                    10.8000                 0\n",
      "73                    10.0000                 0\n",
      "\n",
      "[74 rows x 2 columns]\n",
      "(51, 2) (23, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "projectInterval = pd.read_csv('setapProcessT9.csv')\n",
    "\n",
    "cols = ['averageResponsesByStudent', 'SE Process grade']\n",
    "\n",
    "processesDf = pd.DataFrame(projectInterval, columns=cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "processesDf['SE Process grade'] = label_encoder.fit_transform(processesDf['SE Process grade'])\n",
    "\n",
    "X = processesDf[['averageResponsesByStudent']]\n",
    "y = processesDf['SE Process grade']\n",
    "\n",
    "print(processesDf)\n",
    "\n",
    "train_data, test_data = np.split(processesDf.sample(frac=1, random_state=1729), [int(0.7 * len(processesDf))])\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fd6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data to bucket\n",
    "import os\n",
    "pd.concat([train_data['SE Process grade'], train_data.drop(['SE Process grade'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46170ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.inputs.TrainingInput object at 0x7f0c9b9557e0>\n"
     ]
    }
   ],
   "source": [
    "# test data to bucket\n",
    "import os\n",
    "pd.concat([test_data['SE Process grade'], test_data.drop(['SE Process grade'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = TrainingInput(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')\n",
    "\n",
    "print(s3_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbd2608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.155.0\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker.__version__)\n",
    "# xgb image uyi builds xgb container\n",
    "# container = retrieve(boto3.Session().region_name,\n",
    "#                          'xgboost')\n",
    "# Specify the AWS region\n",
    "region_name = boto3.Session().region_name\n",
    "\n",
    "# Get the container image URI for XGBoost\n",
    "container = image_uris.retrieve('xgboost', region_name, '1.0-1')\n",
    "\n",
    "# Use the 'container' variable for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72af25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"6\",\n",
    "    \"eta\": \"0.005\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"10\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"num_round\": \"50\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6454ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker estimator that calls xgboost\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container,\n",
    "                                         hyperparameters=hyperparameters,\n",
    "                                         role=sagemaker.get_execution_role(),\n",
    "                                         instance_count=1,\n",
    "                                         instance_type='ml.m5.4xlarge',\n",
    "                                         volume_size=5, # gb\n",
    "                                         output_path=output_path,\n",
    "                                         use_spot_instances=True,\n",
    "                                         max_run=300,\n",
    "                                         max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed529c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-06-11-20-14-31-647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://seteamperformance/xgboost-as-a-built-in-algo/train\n",
      "s3://seteamperformance/xgboost-as-a-built-in-algo/test\n",
      "2023-06-11 20:14:31 Starting - Starting the training job...\n",
      "2023-06-11 20:14:45 Starting - Preparing the instances for training...\n",
      "2023-06-11 20:15:29 Downloading - Downloading input data......\n",
      "2023-06-11 20:16:20 Training - Training image download completed. Training in progress..\u001b[34m[2023-06-11 20:16:27.582 ip-10-0-221-53.us-east-2.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter eval_metric value logloss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[20:16:27] 51x1 matrix with 51 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[20:16:27] 23x1 matrix with 23 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2023-06-11 20:16:27.668 ip-10-0-221-53.us-east-2.compute.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-06-11 20:16:27.669 ip-10-0-221-53.us-east-2.compute.internal:7 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-06-11 20:16:27.669 ip-10-0-221-53.us-east-2.compute.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-06-11 20:16:27.670 ip-10-0-221-53.us-east-2.compute.internal:7 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-06-11 20:16:27.670 ip-10-0-221-53.us-east-2.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 51 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 23 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.68369#011validation-logloss:0.68037\u001b[0m\n",
      "\u001b[34m[2023-06-11 20:16:27.673 ip-10-0-221-53.us-east-2.compute.internal:7 INFO hook.py:423] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2023-06-11 20:16:27.675 ip-10-0-221-53.us-east-2.compute.internal:7 INFO hook.py:486] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.67323#011validation-logloss:0.66565\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.66430#011validation-logloss:0.65222\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.65949#011validation-logloss:0.64441\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.65781#011validation-logloss:0.64151\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.65597#011validation-logloss:0.63818\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.65481#011validation-logloss:0.63598\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.65243#011validation-logloss:0.63103\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.65011#011validation-logloss:0.62485\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.65008#011validation-logloss:0.62477\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.64952#011validation-logloss:0.62242\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.64932#011validation-logloss:0.62104\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.64929#011validation-logloss:0.62081\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.64928#011validation-logloss:0.62063\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.64954#011validation-logloss:0.62251\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.64937#011validation-logloss:0.62151\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.64947#011validation-logloss:0.62214\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.64930#011validation-logloss:0.62086\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.64936#011validation-logloss:0.62144\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.64949#011validation-logloss:0.62225\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.64931#011validation-logloss:0.62094\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.64925#011validation-logloss:0.61996\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.64927#011validation-logloss:0.61921\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.64929#011validation-logloss:0.61895\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.64958#011validation-logloss:0.61751\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.64998#011validation-logloss:0.61663\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.65000#011validation-logloss:0.61659\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.64925#011validation-logloss:0.61978\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.64933#011validation-logloss:0.62113\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.64927#011validation-logloss:0.62052\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.65013#011validation-logloss:0.62491\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.65010#011validation-logloss:0.62482\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.64934#011validation-logloss:0.62125\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.64926#011validation-logloss:0.62018\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.64928#011validation-logloss:0.61901\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.64954#011validation-logloss:0.61761\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.64945#011validation-logloss:0.61796\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.64928#011validation-logloss:0.62060\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.64926#011validation-logloss:0.62038\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.64938#011validation-logloss:0.61828\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.64954#011validation-logloss:0.61764\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.64945#011validation-logloss:0.61794\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.64926#011validation-logloss:0.62035\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.64926#011validation-logloss:0.62033\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.64925#011validation-logloss:0.61984\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.64933#011validation-logloss:0.62119\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.64970#011validation-logloss:0.62331\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.64941#011validation-logloss:0.62179\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.64941#011validation-logloss:0.62177\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.64944#011validation-logloss:0.62196\u001b[0m\n",
      "\n",
      "2023-06-11 20:16:46 Uploading - Uploading generated training model\n",
      "2023-06-11 20:16:46 Completed - Training job completed\n",
      "Training seconds: 77\n",
      "Billable seconds: 28\n",
      "Managed Spot Training savings: 63.6%\n"
     ]
    }
   ],
   "source": [
    "# print(s3_input_train.config, s3_input_test.config)\n",
    "print(s3_input_train.config['DataSource']['S3DataSource']['S3Uri'])\n",
    "print(s3_input_test.config['DataSource']['S3DataSource']['S3Uri'])\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55385469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-06-11-20-17-13-478\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2023-06-11-20-17-13-478\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2023-06-11-20-17-13-478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2ea2815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.6   ]\n",
      " [ 9.6667]\n",
      " [11.6667]\n",
      " [10.1429]\n",
      " [12.    ]\n",
      " [ 9.8   ]\n",
      " [11.1667]\n",
      " [11.    ]\n",
      " [10.7143]\n",
      " [11.    ]\n",
      " [ 8.6667]\n",
      " [11.    ]\n",
      " [10.1667]\n",
      " [11.    ]\n",
      " [10.8   ]\n",
      " [12.    ]\n",
      " [11.2   ]\n",
      " [10.8333]\n",
      " [10.    ]\n",
      " [12.    ]\n",
      " [10.3333]\n",
      " [12.    ]\n",
      " [10.7143]]\n",
      "[11.6667]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.36229271, 0.36229271, 0.36229271, 0.36229271, 0.36229271,\n",
       "       0.36229271, 0.36229271, 0.36229271, 0.36229271, 0.36229271,\n",
       "       0.36229271, 0.36229271, 0.36229271, 0.36229271, 0.36229271,\n",
       "       0.36229271, 0.36229271, 0.36229271, 0.36229271, 0.36229271,\n",
       "       0.36229271, 0.36229271, 0.36229271])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "test_data_array = test_data.drop(['SE Process grade'], axis=1).values\n",
    "# test_data_array = test_data['averageResponsesByStudent'].values\n",
    "\n",
    "print(test_data_array)\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "\n",
    "print(test_data_array[2])\n",
    "\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8')\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73a7527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoints\n",
    "\n",
    "# sagemaker.Session().delete_endpoint(xgb_predictor.endpoint_name)\n",
    "# bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "# bucket_to_delete.objects.all().delete()\n",
    "\n",
    "# s3_client = boto3.client('s3')\n",
    "# s3_client.delete_bucket(Bucket=bucket_name)\n",
    "\n",
    "# need to delete correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb0f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
